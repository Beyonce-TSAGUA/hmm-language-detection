ğŸŒğŸ”¤ DÃ©tection automatique de la langue avec les ModÃ¨les de Markov CachÃ©s (HMM)
ğŸ“Œ PrÃ©sentation du projet
Ce projet met en Å“uvre un systÃ¨me de reconnaissance automatique de la langue basÃ© sur les ModÃ¨les de Markov CachÃ©s (Hidden Markov Models â€“ HMM).
ğŸ¯ Lâ€™objectif : identifier la langue dâ€™un mot ou dâ€™un texte en exploitant les rÃ©gularitÃ©s statistiques des sÃ©quences de lettres.

Ce dÃ©pÃ´t a Ã©tÃ© conÃ§u pour mettre en valeur des compÃ©tences en modÃ©lisation probabiliste, algorithmique et Python, dans un contexte proche des problÃ©matiques rÃ©elles du Traitement Automatique du Langage (TAL/NLP).

ğŸ¯ Objectifs techniques
ğŸ§  ImplÃ©menter un modÃ¨le probabiliste HMM from scratch

ğŸ”¡ Analyser des sÃ©quences de caractÃ¨res pour la classification linguistique

âš–ï¸ Comparer diffÃ©rentes stratÃ©gies de modÃ©lisation et mesurer leurs performances

ğŸ“ Produire une analyse critique des rÃ©sultats obtenus

ğŸ§© CompÃ©tences mises en avant
ğŸ“Š ModÃ©lisation statistique (HMM)

ğŸ” Algorithmes probabilistes : Forward / Backward

ğŸ§¬ Analyse de sÃ©quences

ğŸ§® Calcul matriciel & algÃ¨bre linÃ©aire

ğŸ“‰ Ã‰valuation de modÃ¨les (matrices de confusion)

ğŸ Python scientifique

ğŸ› ï¸ Technologies utilisÃ©es
ğŸ Python

ğŸ”¢ NumPy â€“ calcul matriciel

ğŸ—‚ï¸ Pandas â€“ manipulation de donnÃ©es

ğŸ“ˆ Matplotlib â€“ visualisation

âš™ï¸ SciPy â€“ outils numÃ©riques

ğŸ§ª DÃ©marche et mÃ©thodologie
1ï¸âƒ£ PrÃ©traitement des donnÃ©es
ğŸ§¹ Nettoyage des corpus textuels

ğŸ”¤ Normalisation (minuscules, suppression des accents, caractÃ¨res spÃ©ciaux)

ğŸ” Conversion des mots en sÃ©quences de lettres (aâ€“z)

2ï¸âƒ£ Construction du modÃ¨le HMM
Un modÃ¨le HMM est construit pour chaque langue :

ğŸ”€ Matrice de transition : probabilitÃ© de passage entre lettres

ğŸ¯ Matrice dâ€™Ã©mission : probabilitÃ© dâ€™Ã©mission des symboles

ğŸš€ Vecteur de probabilitÃ© initiale

Chaque langue est reprÃ©sentÃ©e par un modÃ¨le statistique distinct.

3ï¸âƒ£ InfÃ©rence probabiliste
âš™ï¸ ImplÃ©mentation des algorithmes Forward et Backward

ğŸ“Š Calcul de la probabilitÃ© quâ€™un mot/texte appartienne Ã  une langue

ğŸ† SÃ©lection de la langue la plus probable

4ï¸âƒ£ Ã‰valuation et analyse
ğŸ§ª Classification mot par mot et texte par texte

ğŸ§© Construction de matrices de confusion

ğŸ” Analyse de lâ€™impact :

de la longueur des mots

de la structure interne des sÃ©quences

de la matrice dâ€™Ã©mission

â­ RÃ©sultats clÃ©s
ğŸ“ Les mots longs sont beaucoup mieux classÃ©s

â“ Les mots courts sont plus ambigus

ğŸ¯ La matrice dâ€™Ã©mission influence fortement les performances

âš ï¸ Une matrice dâ€™Ã©mission identitÃ© â†’ forte baisse de prÃ©cision

ğŸ’¼ Valeur pour un recruteur
Ce projet dÃ©montre :

ğŸ§  Une capacitÃ© Ã  implÃ©menter des modÃ¨les mathÃ©matiques complexes

ğŸ“š Une maÃ®trise solide des fondements probabilistes

ğŸ§ª Une approche rigoureuse de lâ€™Ã©valuation de modÃ¨les

ğŸ§ Une aptitude Ã  analyser et expliquer les limites dâ€™un systÃ¨me

ğŸš€ Des compÃ©tences transfÃ©rables vers le Machine Learning, le NLP et la Data Science

ğŸš€ Pistes dâ€™amÃ©lioration
ğŸ“š Enrichissement des corpus dâ€™apprentissage

ğŸŒ Ajout de nouvelles langues

âš™ï¸ Optimisation des paramÃ¨tres du modÃ¨le

ğŸ¤– Introduction dâ€™algorithmes dâ€™apprentissage (Baum-Welch)

âœï¸ Auteur
TSAGUA YEMEWA BeyoncÃ©